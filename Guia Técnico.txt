# Guia Técnico: Migração langchain-ollama, WAHA e Docker Management

Esta análise técnica fornece informações precisas e exemplos práticos para migração do langchain-community para langchain-ollama, implementação WAHA com Docker, e gerenciamento robusto de containers em Python.

## Migração langchain-community → langchain-ollama

### Instalação e importação correta

**Instalação:**
```bash
# Instalar o novo pacote
pip install -U langchain-ollama

# Opcional: CLI para migração automatizada
pip install -U langchain-cli
langchain-cli migrate path/to/your/code
```

**Migração de imports:**
```python
# ❌ Antigo (langchain-community)
from langchain_community.llms.ollama import Ollama

# ✅ Novo (langchain-ollama)  
from langchain_ollama.llms import OllamaLLM
# OU para chat models:
from langchain_ollama import ChatOllama
# OU para embeddings:
from langchain_ollama import OllamaEmbeddings
```

### Sintaxe OllamaLLM e parâmetros suportados

**Parâmetros completos do construtor:**

```python
from langchain_ollama.llms import OllamaLLM

llm = OllamaLLM(
    # ✅ Obrigatório
    model="llama3.1",                              # Nome do modelo
    
    # 🌐 Conexão e Cliente  
    base_url="http://localhost:11434",             # URL base do servidor Ollama
    client_kwargs={},                              # Kwargs para httpx clients
    sync_client_kwargs={},                         # Kwargs para cliente síncrono
    async_client_kwargs={},                        # Kwargs para cliente assíncrono
    
    # 🎛️ Parâmetros do Modelo
    temperature=0.7,                               # Criatividade (default: 0.8)
    num_predict=1000,                              # Max tokens (-1=infinito, -2=fill context)
    num_ctx=4096,                                  # Tamanho da janela de contexto
    num_gpu=None,                                  # Número de GPUs
    num_thread=None,                               # Threads CPU para computação
    
    # 🎯 Sampling Parameters
    top_k=50,                                      # Controle de diversidade (default: 40)
    top_p=0.9,                                     # Nucleus sampling (default: 0.9)
    tfs_z=1.0,                                     # Tail free sampling
    seed=42,                                       # Seed para reprodutibilidade
    
    # 🔄 Controle de Repetição
    repeat_penalty=1.1,                            # Penalidade de repetição
    repeat_last_n=64,                              # Distância look-back
    
    # 🧠 Mirostat Sampling  
    mirostat=0,                                    # 0=disabled, 1=Mirostat, 2=Mirostat 2.0
    mirostat_eta=0.1,                              # Taxa de aprendizado
    mirostat_tau=5.0,                              # Balanço coerência/diversidade
    
    # ⚙️ Controle e Formatação
    stop=["Human:", "AI:"],                        # Tokens de parada
    format="json",                                 # Formato de saída ('', 'json')
    keep_alive="5m",                               # Retenção modelo na memória
    
    # 📊 Cache e Callbacks
    cache=None,                                    # Cache de resposta
    callbacks=None,                                # Callbacks de runtime
    metadata=None,                                 # Metadados de execução
    tags=None,                                     # Tags de execução
    
    # 🔧 Diversos
    verbose=False,                                 # Print response text
    validate_model_on_init=True,                   # Validar existência do modelo (v0.3.4+)
)
```

### **⚠️ Timeout Parameter - Informação Crítica**

**O OllamaLLM NÃO possui parâmetro `timeout` direto.** O controle de timeout é feito via configuração HTTP client:

```python
# ❌ INCORRETO - não existe parâmetro timeout direto
llm = OllamaLLM(model="llama3.1", timeout=120.0)  # ERRO!

# ✅ CORRETO - timeout via client_kwargs
llm = OllamaLLM(
    model="llama3.1",
    client_kwargs={"timeout": 120.0}  # 120 segundos
)

# ✅ AVANÇADO - timeouts separados para sync/async
llm = OllamaLLM(
    model="llama3.1",
    sync_client_kwargs={"timeout": 120.0},
    async_client_kwargs={"timeout": 180.0}
)

# ✅ CONTROLE GRANULAR - usando httpx.Timeout
import httpx

llm = OllamaLLM(
    model="llama3.1",
    client_kwargs={
        "timeout": httpx.Timeout(
            connect=10.0,    # Timeout de conexão
            read=120.0,      # Timeout de leitura  
            write=10.0,      # Timeout de escrita
            pool=10.0        # Timeout de pool
        )
    }
)
```

### Migração step-by-step

**Exemplo prático de migração:**

```python
# 🔴 ANTES (langchain-community)
from langchain_community.llms.ollama import Ollama

llm = Ollama(model="llama3.1")
response = llm("O que é machine learning?")  # Chamada direta
print(response)

# 🟢 DEPOIS (langchain-ollama)
from langchain_ollama.llms import OllamaLLM

llm = OllamaLLM(
    model="llama3.1",
    client_kwargs={"timeout": 60.0}  # Adicionar timeout se necessário
)
response = llm.invoke("O que é machine learning?")  # Usar invoke()
print(response)
```

**Migração com configurações avançadas:**

```python
# 🔴 ANTES
from langchain_community.llms.ollama import Ollama
from langchain_core.prompts import PromptTemplate

llm = Ollama(
    model="llama3.1", 
    base_url="http://localhost:11434"
)
prompt = PromptTemplate.from_template("Pergunta: {question}\nResposta:")
chain = prompt | llm

# 🟢 DEPOIS  
from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import PromptTemplate

llm = OllamaLLM(
    model="llama3.1",
    base_url="http://localhost:11434",
    temperature=0.1,                    # Consistência em produção
    num_ctx=8192,                       # Contexto maior
    client_kwargs={"timeout": 300.0},   # 5 minutos timeout
    keep_alive="30m",                   # Manter modelo carregado
    validate_model_on_init=True         # Validação antecipada
)
prompt = PromptTemplate.from_template("Pergunta: {question}\nResposta:")
chain = prompt | llm  # Chain continua funcionando igual
```

### Diferenças na API e métodos disponíveis

**Principais mudanças:**

| Aspecto | langchain-community | langchain-ollama |
|---------|--------------------|--------------------|
| **Import** | `from langchain_community.llms.ollama import Ollama` | `from langchain_ollama.llms import OllamaLLM` |
| **Classe** | `Ollama` | `OllamaLLM` |
| **Chamada direta** | `llm("prompt")` ✅ | `llm("prompt")` ❌ (deprecated) |
| **Invoke** | `llm.invoke("prompt")` ✅ | `llm.invoke("prompt")` ✅ (preferred) |
| **Timeout** | Não suportado | Via `client_kwargs` |
| **Validação modelo** | Manual | `validate_model_on_init=True` |

**Métodos compatíveis:**
```python
# ✅ Todos estes métodos funcionam em ambas versões
response = llm.invoke("prompt")
stream = llm.stream("prompt")  
batch = llm.batch(["prompt1", "prompt2"])
async_response = await llm.ainvoke("prompt")
async_stream = llm.astream("prompt")
```

### Métodos assíncronos (ainvoke, etc.)

**Exemplos práticos de uso async:**

```python
import asyncio
from langchain_ollama import OllamaLLM

async def exemplo_basico():
    llm = OllamaLLM(
        model="llama3.1",
        temperature=0.7,
        client_kwargs={"timeout": 60.0}
    )
    
    # Invocação assíncrona básica
    response = await llm.ainvoke("Explique Python")
    print(response)

async def exemplo_streaming():
    llm = OllamaLLM(model="llama3.1")
    
    # Streaming assíncrono
    async for chunk in llm.astream("Conte uma história longa"):
        print(chunk, end="", flush=True)

async def exemplo_batch():
    llm = OllamaLLM(model="llama3.1")
    
    prompts = [
        "O que é IA?",
        "Explique machine learning",
        "O que é deep learning?"
    ]
    
    # Processamento em lote assíncrono
    results = await llm.abatch(prompts)
    for i, result in enumerate(results):
        print(f"Resposta {i+1}: {result[:100]}...")

# Executar exemplos
asyncio.run(exemplo_basico())
```

**Padrão concorrente para múltiplas queries:**

```python
async def processamento_concorrente():
    llm = OllamaLLM(
        model="llama3.1",
        client_kwargs={"timeout": 120.0}
    )
    
    async def processar_query(query, id_query):
        try:
            resultado = await llm.ainvoke(f"Pergunta: {query}")
            return {"id": id_query, "resultado": resultado, "status": "sucesso"}
        except Exception as e:
            return {"id": id_query, "erro": str(e), "status": "erro"}
    
    # Executar múltiplas queries em paralelo
    queries = ["IA", "ML", "DL", "NLP", "Computer Vision"]
    tasks = [processar_query(q, i) for i, q in enumerate(queries)]
    
    resultados = await asyncio.gather(*tasks, return_exceptions=True)
    return resultados
```

### Compatibilidade Python 3.10+

**Requisitos de versão:**

```python
# Verificação de compatibilidade
import sys
assert sys.version_info >= (3, 8), "Python 3.8+ obrigatório"

# langchain-ollama requer:
# Python: 3.8+ (3.10+ recomendado)
# langchain-core: 0.2.0+
# Ollama Server: 0.2.1+
```

**Matriz de compatibilidade:**

| Componente | Mínimo | Recomendado | Notas |
|------------|---------|-------------|--------|
| Python | 3.8 | 3.10+ | Python 3.7 não suportado |
| langchain-core | 0.2.0 | Latest | Auto-instalado |
| langchain-ollama | 0.3.0 | 0.3.6+ | Versão estável |
| Ollama Server | 0.2.1 | Latest | Para client.chat() method |

## WAHA (WhatsApp HTTP API) e Docker

### Comandos Docker corretos

**Comandos básicos:**

```bash
# 🚀 Quick start (desenvolvimento)
docker run -it --rm -p 3000:3000 --name waha devlikeapro/waha

# 💾 Com persistência de sessão
docker run -it -v $(pwd)/sessions:/app/.sessions \
  --rm -p 3000:3000 --name waha devlikeapro/waha

# 🔗 Com webhooks
docker run -it --rm --network=host \
  -e WHATSAPP_HOOK_URL=http://localhost:5000/webhook \
  -e "WHATSAPP_HOOK_EVENTS=message,state.change" \
  --name waha devlikeapro/waha

# 🍎 Para Apple M1/M2 (ARM)
docker pull devlikeapro/waha:arm
docker tag devlikeapro/waha:arm devlikeapro/waha
docker run -it --rm -p 3000:3000 --name waha devlikeapro/waha
```

**Docker Compose para produção:**

```yaml
version: '3.8'

services:
  waha:
    image: devlikeapro/waha-plus  # Versão Plus (requer licença)
    container_name: waha-prod
    restart: unless-stopped
    ports:
      - "127.0.0.1:3000:3000"  # Bind apenas localhost
    volumes:
      - ./sessions:/app/.sessions
      - ./media:/tmp/whatsapp-files
    environment:
      # 🔐 Segurança
      - WAHA_API_KEY=sha512:${WAHA_API_KEY_HASH}
      - WAHA_DASHBOARD_USERNAME=admin
      - WAHA_DASHBOARD_PASSWORD=${DASHBOARD_PASSWORD}
      
      # ⚙️ Configuração
      - WHATSAPP_API_PORT=3000
      - WAHA_BASE_URL=https://yourdomain.com
      - TZ=America/Sao_Paulo
      
      # 🔄 Gerenciamento de sessão
      - WAHA_WORKER_ID=waha-prod-br
      - WHATSAPP_RESTART_ALL_SESSIONS=true
      - WAHA_AUTO_START_DELAY_SECONDS=5
      - WAHA_PRINT_QR=false
      
      # 🤖 Engine (WEBJS/NOWEB/GOWS)
      - WHATSAPP_DEFAULT_ENGINE=WEBJS
      
      # 📝 Logging
      - WAHA_LOG_LEVEL=info
      - WAHA_LOG_FORMAT=JSON
      
      # 🔔 Webhooks
      - WHATSAPP_HOOK_URL=https://meuapp.com/webhook
      - WHATSAPP_HOOK_EVENTS=message,message.any,state.change
      - WHATSAPP_HOOK_HMAC_KEY=${WEBHOOK_SECRET}
      
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2'
        reservations:
          memory: 2G
          cpus: '1'
```

**Arquivo .env:**

```bash
# Gerar API key segura
WAHA_API_KEY_HASH=98b6d128682e280b74b324ca82a6bae6e8a3f7174e0605bfd52eb9948fad8984854ec08f7652f32055c4a9f12b69add4850481d9503a7f2225501671d6124648

# Senhas fortes
DASHBOARD_PASSWORD=SuaSenhaForte123!
WEBHOOK_SECRET=SeuSegredoWebhook456!
```

### Cliente Python para WAHA

```python
import requests
import json
from typing import Dict, Any, Optional

class WAHAClient:
    def __init__(self, base_url: str, api_key: str):
        self.base_url = base_url.rstrip('/')
        self.headers = {
            'X-Api-Key': api_key,
            'Content-Type': 'application/json'
        }
    
    def criar_sessao(self, nome_sessao: str, config: Optional[Dict] = None) -> Dict:
        """Criar e iniciar uma sessão do WhatsApp"""
        data = {'name': nome_sessao}
        if config:
            data['config'] = config
        
        response = requests.post(
            f"{self.base_url}/api/sessions",
            headers=self.headers,
            json=data
        )
        response.raise_for_status()
        return response.json()
    
    def obter_qr_code(self, sessao: str = 'default') -> bytes:
        """Obter screenshot do QR code"""
        response = requests.get(
            f"{self.base_url}/api/screenshot?session={sessao}",
            headers={'X-Api-Key': self.headers['X-Api-Key']}
        )
        response.raise_for_status()
        return response.content
    
    def enviar_texto(self, chat_id: str, texto: str, sessao: str = 'default') -> Dict:
        """Enviar mensagem de texto"""
        data = {
            'chatId': chat_id,
            'text': texto,
            'session': sessao
        }
        response = requests.post(
            f"{self.base_url}/api/sendText",
            headers=self.headers,
            json=data
        )
        response.raise_for_status()
        return response.json()

# 📱 Uso prático
client = WAHAClient('http://localhost:3000', 'sua-api-key')

# Criar sessão com webhook
config_sessao = {
    'webhook': {
        'url': 'https://meuapp.com/webhook',
        'events': ['message', 'state.change']
    }
}
client.criar_sessao('default', config_sessao)

# Enviar mensagem
client.enviar_texto('5511999999999@c.us', 'Olá do Python!')
```

## Melhores práticas Docker em Python

### Gerenciamento robusto de containers

```python
import docker
import logging
from contextlib import contextmanager
from tenacity import retry, stop_after_attempt, wait_exponential

class GerenciadorContainerRobusto:
    def __init__(self):
        self.client = docker.from_env()
        self.logger = logging.getLogger(__name__)
    
    @contextmanager
    def container_gerenciado(self, imagem, **kwargs):
        """Context manager para lifecycle de container"""
        container = None
        try:
            container = self.client.containers.run(
                imagem, detach=True, **kwargs
            )
            yield container
        except Exception as e:
            self.logger.error(f"Erro no container: {e}")
            raise
        finally:
            if container:
                try:
                    container.stop(timeout=10)
                    container.remove()
                except Exception as e:
                    self.logger.warning(f"Erro ao limpar container: {e}")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    def iniciar_container_com_retry(self, imagem: str, **kwargs):
        """Iniciar container com retry automático"""
        try:
            # Verificar se imagem existe, baixar se necessário
            try:
                self.client.images.get(imagem)
            except docker.errors.ImageNotFound:
                self.logger.info(f"Baixando imagem {imagem}")
                self.client.images.pull(imagem)
            
            # Criar e iniciar container
            container = self.client.containers.run(imagem, detach=True, **kwargs)
            
            # Aguardar health check
            self.aguardar_health_check(container.id)
            
            return container
            
        except docker.errors.APIError as e:
            self.logger.error(f"Erro API Docker: {e}")
            raise
    
    def aguardar_health_check(self, container_id: str, timeout: int = 60):
        """Aguardar container ficar saudável"""
        import time
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            container = self.client.containers.get(container_id)
            health = container.attrs.get('State', {}).get('Health', {})
            status = health.get('Status', 'unknown')
            
            if status == 'healthy':
                return True
            elif status == 'unhealthy':
                raise Exception(f"Container {container_id} não saudável")
            
            time.sleep(5)
        
        raise TimeoutError(f"Health check timeout para {container_id}")
```

### Retry automático e tratamento de erros

**Implementação com Tenacity:**

```python
from tenacity import (
    retry, stop_after_attempt, wait_exponential, 
    retry_if_exception_type, before_log, after_log
)
import logging
import asyncio
import aiohttp

logger = logging.getLogger(__name__)

class SistemaResilienteCompleto:
    
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=2, max=30),
        retry=retry_if_exception_type((
            aiohttp.ClientError,
            asyncio.TimeoutError,
            ConnectionError
        )),
        before=before_log(logger, logging.INFO),
        after=after_log(logger, logging.WARNING)
    )
    async def requisicao_api_robusta(self, url: str, **kwargs):
        """Requisição HTTP com retry e tratamento de erro"""
        timeout = aiohttp.ClientTimeout(total=30)
        
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url, **kwargs) as response:
                if response.status >= 500:
                    # Erro servidor - retry
                    raise aiohttp.ClientResponseError(
                        request_info=response.request_info,
                        history=response.history,
                        status=response.status
                    )
                elif response.status >= 400:
                    # Erro cliente - não retry
                    response.raise_for_status()
                
                return await response.json()
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=2, min=5, max=60)
    )
    def operacao_docker_critica(self, container_id: str):
        """Operação crítica com retry exponential"""
        try:
            container = self.client.containers.get(container_id)
            
            # Operação crítica
            result = container.exec_run("critical-command")
            
            if result.exit_code != 0:
                raise Exception(f"Comando falhou: {result.output}")
            
            return result.output
            
        except docker.errors.NotFound:
            # Não fazer retry se container não existe
            raise
        except docker.errors.APIError as e:
            logger.error(f"Erro API Docker: {e}")
            raise
```

**Circuit Breaker para operações críticas:**

```python
import time
from enum import Enum
from typing import Callable, Any

class EstadoCircuit(Enum):
    FECHADO = "fechado"
    ABERTO = "aberto"
    MEIO_ABERTO = "meio_aberto"

class CircuitBreaker:
    def __init__(self, limite_falhas: int = 5, timeout_recuperacao: int = 60):
        self.limite_falhas = limite_falhas
        self.timeout_recuperacao = timeout_recuperacao
        self.contador_falhas = 0
        self.tempo_ultima_falha = None
        self.estado = EstadoCircuit.FECHADO
    
    def executar(self, func: Callable, *args, **kwargs) -> Any:
        """Executar função com circuit breaker"""
        if self.estado == EstadoCircuit.ABERTO:
            if time.time() - self.tempo_ultima_falha > self.timeout_recuperacao:
                self.estado = EstadoCircuit.MEIO_ABERTO
            else:
                raise Exception("Circuit breaker está aberto")
        
        try:
            resultado = func(*args, **kwargs)
            self.ao_sucesso()
            return resultado
        except Exception as e:
            self.ao_falha()
            raise e
    
    def ao_sucesso(self):
        """Reset circuit breaker no sucesso"""
        self.contador_falhas = 0
        self.estado = EstadoCircuit.FECHADO
    
    def ao_falha(self):
        """Incrementar contador de falhas"""
        self.contador_falhas += 1
        self.tempo_ultima_falha = time.time()
        
        if self.contador_falhas >= self.limite_falhas:
            self.estado = EstadoCircuit.ABERTO

# 🔄 Uso prático
circuit_breaker = CircuitBreaker(limite_falhas=3, timeout_recuperacao=30)

def operacao_instavel():
    # Simular operação que pode falhar
    import random
    if random.random() < 0.7:  # 70% chance de falha
        raise Exception("Operação falhou")
    return "Sucesso!"

# Executar com circuit breaker
try:
    resultado = circuit_breaker.executar(operacao_instavel)
    print(f"Resultado: {resultado}")
except Exception as e:
    print(f"Erro: {e}")
```

### Monitoramento e logging estruturado

```python
import json
import logging
import time
from datetime import datetime

class LoggerEstruturado:
    def __init__(self, nome: str):
        self.logger = logging.getLogger(nome)
        self.configurar_logger()
    
    def configurar_logger(self):
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def log_estruturado(self, level: str, mensagem: str, **kwargs):
        """Gerar log estruturado em JSON"""
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': level.upper(),
            'message': mensagem,
            'service': 'docker-manager',
            **kwargs
        }
        
        getattr(self.logger, level.lower())(json.dumps(log_entry))

# 📊 Monitoramento de containers
class MonitorContainer:
    def __init__(self):
        self.client = docker.from_env()
        self.logger = LoggerEstruturado(__name__)
    
    def coletar_metricas_container(self, container_id: str):
        """Coletar métricas detalhadas do container"""
        try:
            container = self.client.containers.get(container_id)
            stats = container.stats(stream=False)
            
            # Calcular CPU usage
            cpu_stats = stats['cpu_stats']
            precpu_stats = stats['precpu_stats']
            cpu_delta = cpu_stats['cpu_usage']['total_usage'] - precpu_stats['cpu_usage']['total_usage']
            system_delta = cpu_stats['system_cpu_usage'] - precpu_stats['system_cpu_usage']
            cpu_usage = (cpu_delta / system_delta) * len(cpu_stats['cpu_usage']['percpu_usage']) * 100.0
            
            # Memory usage
            memory_stats = stats['memory_stats']
            memory_usage = (memory_stats['usage'] / memory_stats['limit']) * 100.0
            
            metricas = {
                'container_id': container_id,
                'container_name': container.name,
                'cpu_usage_percent': round(cpu_usage, 2),
                'memory_usage_percent': round(memory_usage, 2),
                'memory_usage_mb': round(memory_stats['usage'] / (1024*1024), 2),
                'memory_limit_mb': round(memory_stats['limit'] / (1024*1024), 2),
                'status': container.status
            }
            
            # Log métricas
            self.logger.log_estruturado('info', 'Métricas container coletadas', **metricas)
            
            # Alertas
            if cpu_usage > 80:
                self.logger.log_estruturado('warning', 'Alto uso CPU', container_id=container_id, cpu_usage=cpu_usage)
            if memory_usage > 85:
                self.logger.log_estruturado('warning', 'Alto uso memória', container_id=container_id, memory_usage=memory_usage)
            
            return metricas
            
        except Exception as e:
            self.logger.log_estruturado('error', 'Erro coletando métricas', container_id=container_id, erro=str(e))
            raise
```

## Exemplos práticos completos

### Sistema completo langchain-ollama + WAHA + Docker

```python
import asyncio
import docker
from langchain_ollama import OllamaLLM
from tenacity import retry, stop_after_attempt, wait_exponential
import aiohttp
import logging

class SistemaCompleto:
    def __init__(self):
        self.docker_client = docker.from_env()
        self.logger = logging.getLogger(__name__)
        self.ollama_llm = None
        self.waha_client = None
        
    async def inicializar_sistema(self):
        """Inicializar todos os componentes"""
        # 1. Verificar/iniciar Ollama
        await self.garantir_ollama_disponivel()
        
        # 2. Inicializar LLM
        self.ollama_llm = OllamaLLM(
            model="llama3.1",
            temperature=0.1,
            num_ctx=4096,
            client_kwargs={"timeout": 120.0},
            keep_alive="30m",
            validate_model_on_init=True
        )
        
        # 3. Verificar/iniciar WAHA
        await self.garantir_waha_disponivel()
        
        # 4. Configurar cliente WAHA
        self.waha_client = WAHAClient('http://localhost:3000', 'sua-api-key')
        
        self.logger.info("Sistema completo inicializado")
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=5, max=30))
    async def garantir_ollama_disponivel(self):
        """Garantir que Ollama está rodando"""
        try:
            # Verificar se container Ollama existe
            containers = self.docker_client.containers.list(filters={'name': 'ollama'})
            
            if not containers:
                # Iniciar container Ollama
                self.logger.info("Iniciando container Ollama")
                container = self.docker_client.containers.run(
                    'ollama/ollama:latest',
                    name='ollama',
                    ports={'11434/tcp': 11434},
                    volumes={'ollama_data': {'bind': '/root/.ollama', 'mode': 'rw'}},
                    detach=True,
                    restart_policy={"Name": "unless-stopped"}
                )
                
                # Aguardar Ollama ficar disponível
                await self.aguardar_servico_disponivel('http://localhost:11434/api/tags')
                
                # Baixar modelo se necessário
                await self.garantir_modelo_disponivel('llama3.1')
        
        except Exception as e:
            self.logger.error(f"Erro garantindo Ollama: {e}")
            raise
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=5, max=30))
    async def garantir_waha_disponivel(self):
        """Garantir que WAHA está rodando"""
        try:
            containers = self.docker_client.containers.list(filters={'name': 'waha'})
            
            if not containers:
                self.logger.info("Iniciando container WAHA")
                container = self.docker_client.containers.run(
                    'devlikeapro/waha:latest',
                    name='waha',
                    ports={'3000/tcp': 3000},
                    environment={
                        'WAHA_API_KEY': 'sua-api-key',
                        'WHATSAPP_DEFAULT_ENGINE': 'WEBJS'
                    },
                    volumes={'./sessions': {'bind': '/app/.sessions', 'mode': 'rw'}},
                    detach=True,
                    restart_policy={"Name": "unless-stopped"}
                )
                
                # Aguardar WAHA ficar disponível
                await self.aguardar_servico_disponivel('http://localhost:3000/health')
        
        except Exception as e:
            self.logger.error(f"Erro garantindo WAHA: {e}")
            raise
    
    async def aguardar_servico_disponivel(self, url: str, timeout: int = 60):
        """Aguardar serviço ficar disponível"""
        import time
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(url) as response:
                        if response.status == 200:
                            return True
            except:
                pass
            
            await asyncio.sleep(5)
        
        raise TimeoutError(f"Serviço {url} não ficou disponível")
    
    async def processar_mensagem_whatsapp(self, chat_id: str, mensagem: str):
        """Processar mensagem do WhatsApp com IA"""
        try:
            # Processar com Ollama
            resposta_ia = await self.ollama_llm.ainvoke(
                f"Responda de forma útil e concisa: {mensagem}"
            )
            
            # Enviar resposta via WAHA
            self.waha_client.enviar_texto(chat_id, resposta_ia)
            
            self.logger.info(f"Mensagem processada para {chat_id}")
            return resposta_ia
            
        except Exception as e:
            self.logger.error(f"Erro processando mensagem: {e}")
            # Enviar mensagem de erro
            self.waha_client.enviar_texto(chat_id, "Desculpe, houve um erro interno.")
            raise

# 🚀 Uso do sistema completo
async def main():
    sistema = SistemaCompleto()
    await sistema.inicializar_sistema()
    
    # Exemplo de uso
    resposta = await sistema.processar_mensagem_whatsapp(
        "5511999999999@c.us", 
        "Explique o que é inteligência artificial"
    )
    print(f"Resposta enviada: {resposta[:100]}...")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
```

Este guia fornece informações técnicas precisas e exemplos práticos para implementar uma solução robusta usando langchain-ollama, WAHA e Docker com Python, incluindo retry automático, tratamento de erros e melhores práticas de produção.