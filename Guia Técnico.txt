# Guia T√©cnico: Migra√ß√£o langchain-ollama, WAHA e Docker Management

Esta an√°lise t√©cnica fornece informa√ß√µes precisas e exemplos pr√°ticos para migra√ß√£o do langchain-community para langchain-ollama, implementa√ß√£o WAHA com Docker, e gerenciamento robusto de containers em Python.

## Migra√ß√£o langchain-community ‚Üí langchain-ollama

### Instala√ß√£o e importa√ß√£o correta

**Instala√ß√£o:**
```bash
# Instalar o novo pacote
pip install -U langchain-ollama

# Opcional: CLI para migra√ß√£o automatizada
pip install -U langchain-cli
langchain-cli migrate path/to/your/code
```

**Migra√ß√£o de imports:**
```python
# ‚ùå Antigo (langchain-community)
from langchain_community.llms.ollama import Ollama

# ‚úÖ Novo (langchain-ollama)  
from langchain_ollama.llms import OllamaLLM
# OU para chat models:
from langchain_ollama import ChatOllama
# OU para embeddings:
from langchain_ollama import OllamaEmbeddings
```

### Sintaxe OllamaLLM e par√¢metros suportados

**Par√¢metros completos do construtor:**

```python
from langchain_ollama.llms import OllamaLLM

llm = OllamaLLM(
    # ‚úÖ Obrigat√≥rio
    model="llama3.1",                              # Nome do modelo
    
    # üåê Conex√£o e Cliente  
    base_url="http://localhost:11434",             # URL base do servidor Ollama
    client_kwargs={},                              # Kwargs para httpx clients
    sync_client_kwargs={},                         # Kwargs para cliente s√≠ncrono
    async_client_kwargs={},                        # Kwargs para cliente ass√≠ncrono
    
    # üéõÔ∏è Par√¢metros do Modelo
    temperature=0.7,                               # Criatividade (default: 0.8)
    num_predict=1000,                              # Max tokens (-1=infinito, -2=fill context)
    num_ctx=4096,                                  # Tamanho da janela de contexto
    num_gpu=None,                                  # N√∫mero de GPUs
    num_thread=None,                               # Threads CPU para computa√ß√£o
    
    # üéØ Sampling Parameters
    top_k=50,                                      # Controle de diversidade (default: 40)
    top_p=0.9,                                     # Nucleus sampling (default: 0.9)
    tfs_z=1.0,                                     # Tail free sampling
    seed=42,                                       # Seed para reprodutibilidade
    
    # üîÑ Controle de Repeti√ß√£o
    repeat_penalty=1.1,                            # Penalidade de repeti√ß√£o
    repeat_last_n=64,                              # Dist√¢ncia look-back
    
    # üß† Mirostat Sampling  
    mirostat=0,                                    # 0=disabled, 1=Mirostat, 2=Mirostat 2.0
    mirostat_eta=0.1,                              # Taxa de aprendizado
    mirostat_tau=5.0,                              # Balan√ßo coer√™ncia/diversidade
    
    # ‚öôÔ∏è Controle e Formata√ß√£o
    stop=["Human:", "AI:"],                        # Tokens de parada
    format="json",                                 # Formato de sa√≠da ('', 'json')
    keep_alive="5m",                               # Reten√ß√£o modelo na mem√≥ria
    
    # üìä Cache e Callbacks
    cache=None,                                    # Cache de resposta
    callbacks=None,                                # Callbacks de runtime
    metadata=None,                                 # Metadados de execu√ß√£o
    tags=None,                                     # Tags de execu√ß√£o
    
    # üîß Diversos
    verbose=False,                                 # Print response text
    validate_model_on_init=True,                   # Validar exist√™ncia do modelo (v0.3.4+)
)
```

### **‚ö†Ô∏è Timeout Parameter - Informa√ß√£o Cr√≠tica**

**O OllamaLLM N√ÉO possui par√¢metro `timeout` direto.** O controle de timeout √© feito via configura√ß√£o HTTP client:

```python
# ‚ùå INCORRETO - n√£o existe par√¢metro timeout direto
llm = OllamaLLM(model="llama3.1", timeout=120.0)  # ERRO!

# ‚úÖ CORRETO - timeout via client_kwargs
llm = OllamaLLM(
    model="llama3.1",
    client_kwargs={"timeout": 120.0}  # 120 segundos
)

# ‚úÖ AVAN√áADO - timeouts separados para sync/async
llm = OllamaLLM(
    model="llama3.1",
    sync_client_kwargs={"timeout": 120.0},
    async_client_kwargs={"timeout": 180.0}
)

# ‚úÖ CONTROLE GRANULAR - usando httpx.Timeout
import httpx

llm = OllamaLLM(
    model="llama3.1",
    client_kwargs={
        "timeout": httpx.Timeout(
            connect=10.0,    # Timeout de conex√£o
            read=120.0,      # Timeout de leitura  
            write=10.0,      # Timeout de escrita
            pool=10.0        # Timeout de pool
        )
    }
)
```

### Migra√ß√£o step-by-step

**Exemplo pr√°tico de migra√ß√£o:**

```python
# üî¥ ANTES (langchain-community)
from langchain_community.llms.ollama import Ollama

llm = Ollama(model="llama3.1")
response = llm("O que √© machine learning?")  # Chamada direta
print(response)

# üü¢ DEPOIS (langchain-ollama)
from langchain_ollama.llms import OllamaLLM

llm = OllamaLLM(
    model="llama3.1",
    client_kwargs={"timeout": 60.0}  # Adicionar timeout se necess√°rio
)
response = llm.invoke("O que √© machine learning?")  # Usar invoke()
print(response)
```

**Migra√ß√£o com configura√ß√µes avan√ßadas:**

```python
# üî¥ ANTES
from langchain_community.llms.ollama import Ollama
from langchain_core.prompts import PromptTemplate

llm = Ollama(
    model="llama3.1", 
    base_url="http://localhost:11434"
)
prompt = PromptTemplate.from_template("Pergunta: {question}\nResposta:")
chain = prompt | llm

# üü¢ DEPOIS  
from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import PromptTemplate

llm = OllamaLLM(
    model="llama3.1",
    base_url="http://localhost:11434",
    temperature=0.1,                    # Consist√™ncia em produ√ß√£o
    num_ctx=8192,                       # Contexto maior
    client_kwargs={"timeout": 300.0},   # 5 minutos timeout
    keep_alive="30m",                   # Manter modelo carregado
    validate_model_on_init=True         # Valida√ß√£o antecipada
)
prompt = PromptTemplate.from_template("Pergunta: {question}\nResposta:")
chain = prompt | llm  # Chain continua funcionando igual
```

### Diferen√ßas na API e m√©todos dispon√≠veis

**Principais mudan√ßas:**

| Aspecto | langchain-community | langchain-ollama |
|---------|--------------------|--------------------|
| **Import** | `from langchain_community.llms.ollama import Ollama` | `from langchain_ollama.llms import OllamaLLM` |
| **Classe** | `Ollama` | `OllamaLLM` |
| **Chamada direta** | `llm("prompt")` ‚úÖ | `llm("prompt")` ‚ùå (deprecated) |
| **Invoke** | `llm.invoke("prompt")` ‚úÖ | `llm.invoke("prompt")` ‚úÖ (preferred) |
| **Timeout** | N√£o suportado | Via `client_kwargs` |
| **Valida√ß√£o modelo** | Manual | `validate_model_on_init=True` |

**M√©todos compat√≠veis:**
```python
# ‚úÖ Todos estes m√©todos funcionam em ambas vers√µes
response = llm.invoke("prompt")
stream = llm.stream("prompt")  
batch = llm.batch(["prompt1", "prompt2"])
async_response = await llm.ainvoke("prompt")
async_stream = llm.astream("prompt")
```

### M√©todos ass√≠ncronos (ainvoke, etc.)

**Exemplos pr√°ticos de uso async:**

```python
import asyncio
from langchain_ollama import OllamaLLM

async def exemplo_basico():
    llm = OllamaLLM(
        model="llama3.1",
        temperature=0.7,
        client_kwargs={"timeout": 60.0}
    )
    
    # Invoca√ß√£o ass√≠ncrona b√°sica
    response = await llm.ainvoke("Explique Python")
    print(response)

async def exemplo_streaming():
    llm = OllamaLLM(model="llama3.1")
    
    # Streaming ass√≠ncrono
    async for chunk in llm.astream("Conte uma hist√≥ria longa"):
        print(chunk, end="", flush=True)

async def exemplo_batch():
    llm = OllamaLLM(model="llama3.1")
    
    prompts = [
        "O que √© IA?",
        "Explique machine learning",
        "O que √© deep learning?"
    ]
    
    # Processamento em lote ass√≠ncrono
    results = await llm.abatch(prompts)
    for i, result in enumerate(results):
        print(f"Resposta {i+1}: {result[:100]}...")

# Executar exemplos
asyncio.run(exemplo_basico())
```

**Padr√£o concorrente para m√∫ltiplas queries:**

```python
async def processamento_concorrente():
    llm = OllamaLLM(
        model="llama3.1",
        client_kwargs={"timeout": 120.0}
    )
    
    async def processar_query(query, id_query):
        try:
            resultado = await llm.ainvoke(f"Pergunta: {query}")
            return {"id": id_query, "resultado": resultado, "status": "sucesso"}
        except Exception as e:
            return {"id": id_query, "erro": str(e), "status": "erro"}
    
    # Executar m√∫ltiplas queries em paralelo
    queries = ["IA", "ML", "DL", "NLP", "Computer Vision"]
    tasks = [processar_query(q, i) for i, q in enumerate(queries)]
    
    resultados = await asyncio.gather(*tasks, return_exceptions=True)
    return resultados
```

### Compatibilidade Python 3.10+

**Requisitos de vers√£o:**

```python
# Verifica√ß√£o de compatibilidade
import sys
assert sys.version_info >= (3, 8), "Python 3.8+ obrigat√≥rio"

# langchain-ollama requer:
# Python: 3.8+ (3.10+ recomendado)
# langchain-core: 0.2.0+
# Ollama Server: 0.2.1+
```

**Matriz de compatibilidade:**

| Componente | M√≠nimo | Recomendado | Notas |
|------------|---------|-------------|--------|
| Python | 3.8 | 3.10+ | Python 3.7 n√£o suportado |
| langchain-core | 0.2.0 | Latest | Auto-instalado |
| langchain-ollama | 0.3.0 | 0.3.6+ | Vers√£o est√°vel |
| Ollama Server | 0.2.1 | Latest | Para client.chat() method |

## WAHA (WhatsApp HTTP API) e Docker

### Comandos Docker corretos

**Comandos b√°sicos:**

```bash
# üöÄ Quick start (desenvolvimento)
docker run -it --rm -p 3000:3000 --name waha devlikeapro/waha

# üíæ Com persist√™ncia de sess√£o
docker run -it -v $(pwd)/sessions:/app/.sessions \
  --rm -p 3000:3000 --name waha devlikeapro/waha

# üîó Com webhooks
docker run -it --rm --network=host \
  -e WHATSAPP_HOOK_URL=http://localhost:5000/webhook \
  -e "WHATSAPP_HOOK_EVENTS=message,state.change" \
  --name waha devlikeapro/waha

# üçé Para Apple M1/M2 (ARM)
docker pull devlikeapro/waha:arm
docker tag devlikeapro/waha:arm devlikeapro/waha
docker run -it --rm -p 3000:3000 --name waha devlikeapro/waha
```

**Docker Compose para produ√ß√£o:**

```yaml
version: '3.8'

services:
  waha:
    image: devlikeapro/waha-plus  # Vers√£o Plus (requer licen√ßa)
    container_name: waha-prod
    restart: unless-stopped
    ports:
      - "127.0.0.1:3000:3000"  # Bind apenas localhost
    volumes:
      - ./sessions:/app/.sessions
      - ./media:/tmp/whatsapp-files
    environment:
      # üîê Seguran√ßa
      - WAHA_API_KEY=sha512:${WAHA_API_KEY_HASH}
      - WAHA_DASHBOARD_USERNAME=admin
      - WAHA_DASHBOARD_PASSWORD=${DASHBOARD_PASSWORD}
      
      # ‚öôÔ∏è Configura√ß√£o
      - WHATSAPP_API_PORT=3000
      - WAHA_BASE_URL=https://yourdomain.com
      - TZ=America/Sao_Paulo
      
      # üîÑ Gerenciamento de sess√£o
      - WAHA_WORKER_ID=waha-prod-br
      - WHATSAPP_RESTART_ALL_SESSIONS=true
      - WAHA_AUTO_START_DELAY_SECONDS=5
      - WAHA_PRINT_QR=false
      
      # ü§ñ Engine (WEBJS/NOWEB/GOWS)
      - WHATSAPP_DEFAULT_ENGINE=WEBJS
      
      # üìù Logging
      - WAHA_LOG_LEVEL=info
      - WAHA_LOG_FORMAT=JSON
      
      # üîî Webhooks
      - WHATSAPP_HOOK_URL=https://meuapp.com/webhook
      - WHATSAPP_HOOK_EVENTS=message,message.any,state.change
      - WHATSAPP_HOOK_HMAC_KEY=${WEBHOOK_SECRET}
      
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2'
        reservations:
          memory: 2G
          cpus: '1'
```

**Arquivo .env:**

```bash
# Gerar API key segura
WAHA_API_KEY_HASH=98b6d128682e280b74b324ca82a6bae6e8a3f7174e0605bfd52eb9948fad8984854ec08f7652f32055c4a9f12b69add4850481d9503a7f2225501671d6124648

# Senhas fortes
DASHBOARD_PASSWORD=SuaSenhaForte123!
WEBHOOK_SECRET=SeuSegredoWebhook456!
```

### Cliente Python para WAHA

```python
import requests
import json
from typing import Dict, Any, Optional

class WAHAClient:
    def __init__(self, base_url: str, api_key: str):
        self.base_url = base_url.rstrip('/')
        self.headers = {
            'X-Api-Key': api_key,
            'Content-Type': 'application/json'
        }
    
    def criar_sessao(self, nome_sessao: str, config: Optional[Dict] = None) -> Dict:
        """Criar e iniciar uma sess√£o do WhatsApp"""
        data = {'name': nome_sessao}
        if config:
            data['config'] = config
        
        response = requests.post(
            f"{self.base_url}/api/sessions",
            headers=self.headers,
            json=data
        )
        response.raise_for_status()
        return response.json()
    
    def obter_qr_code(self, sessao: str = 'default') -> bytes:
        """Obter screenshot do QR code"""
        response = requests.get(
            f"{self.base_url}/api/screenshot?session={sessao}",
            headers={'X-Api-Key': self.headers['X-Api-Key']}
        )
        response.raise_for_status()
        return response.content
    
    def enviar_texto(self, chat_id: str, texto: str, sessao: str = 'default') -> Dict:
        """Enviar mensagem de texto"""
        data = {
            'chatId': chat_id,
            'text': texto,
            'session': sessao
        }
        response = requests.post(
            f"{self.base_url}/api/sendText",
            headers=self.headers,
            json=data
        )
        response.raise_for_status()
        return response.json()

# üì± Uso pr√°tico
client = WAHAClient('http://localhost:3000', 'sua-api-key')

# Criar sess√£o com webhook
config_sessao = {
    'webhook': {
        'url': 'https://meuapp.com/webhook',
        'events': ['message', 'state.change']
    }
}
client.criar_sessao('default', config_sessao)

# Enviar mensagem
client.enviar_texto('5511999999999@c.us', 'Ol√° do Python!')
```

## Melhores pr√°ticas Docker em Python

### Gerenciamento robusto de containers

```python
import docker
import logging
from contextlib import contextmanager
from tenacity import retry, stop_after_attempt, wait_exponential

class GerenciadorContainerRobusto:
    def __init__(self):
        self.client = docker.from_env()
        self.logger = logging.getLogger(__name__)
    
    @contextmanager
    def container_gerenciado(self, imagem, **kwargs):
        """Context manager para lifecycle de container"""
        container = None
        try:
            container = self.client.containers.run(
                imagem, detach=True, **kwargs
            )
            yield container
        except Exception as e:
            self.logger.error(f"Erro no container: {e}")
            raise
        finally:
            if container:
                try:
                    container.stop(timeout=10)
                    container.remove()
                except Exception as e:
                    self.logger.warning(f"Erro ao limpar container: {e}")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    def iniciar_container_com_retry(self, imagem: str, **kwargs):
        """Iniciar container com retry autom√°tico"""
        try:
            # Verificar se imagem existe, baixar se necess√°rio
            try:
                self.client.images.get(imagem)
            except docker.errors.ImageNotFound:
                self.logger.info(f"Baixando imagem {imagem}")
                self.client.images.pull(imagem)
            
            # Criar e iniciar container
            container = self.client.containers.run(imagem, detach=True, **kwargs)
            
            # Aguardar health check
            self.aguardar_health_check(container.id)
            
            return container
            
        except docker.errors.APIError as e:
            self.logger.error(f"Erro API Docker: {e}")
            raise
    
    def aguardar_health_check(self, container_id: str, timeout: int = 60):
        """Aguardar container ficar saud√°vel"""
        import time
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            container = self.client.containers.get(container_id)
            health = container.attrs.get('State', {}).get('Health', {})
            status = health.get('Status', 'unknown')
            
            if status == 'healthy':
                return True
            elif status == 'unhealthy':
                raise Exception(f"Container {container_id} n√£o saud√°vel")
            
            time.sleep(5)
        
        raise TimeoutError(f"Health check timeout para {container_id}")
```

### Retry autom√°tico e tratamento de erros

**Implementa√ß√£o com Tenacity:**

```python
from tenacity import (
    retry, stop_after_attempt, wait_exponential, 
    retry_if_exception_type, before_log, after_log
)
import logging
import asyncio
import aiohttp

logger = logging.getLogger(__name__)

class SistemaResilienteCompleto:
    
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=2, max=30),
        retry=retry_if_exception_type((
            aiohttp.ClientError,
            asyncio.TimeoutError,
            ConnectionError
        )),
        before=before_log(logger, logging.INFO),
        after=after_log(logger, logging.WARNING)
    )
    async def requisicao_api_robusta(self, url: str, **kwargs):
        """Requisi√ß√£o HTTP com retry e tratamento de erro"""
        timeout = aiohttp.ClientTimeout(total=30)
        
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url, **kwargs) as response:
                if response.status >= 500:
                    # Erro servidor - retry
                    raise aiohttp.ClientResponseError(
                        request_info=response.request_info,
                        history=response.history,
                        status=response.status
                    )
                elif response.status >= 400:
                    # Erro cliente - n√£o retry
                    response.raise_for_status()
                
                return await response.json()
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=2, min=5, max=60)
    )
    def operacao_docker_critica(self, container_id: str):
        """Opera√ß√£o cr√≠tica com retry exponential"""
        try:
            container = self.client.containers.get(container_id)
            
            # Opera√ß√£o cr√≠tica
            result = container.exec_run("critical-command")
            
            if result.exit_code != 0:
                raise Exception(f"Comando falhou: {result.output}")
            
            return result.output
            
        except docker.errors.NotFound:
            # N√£o fazer retry se container n√£o existe
            raise
        except docker.errors.APIError as e:
            logger.error(f"Erro API Docker: {e}")
            raise
```

**Circuit Breaker para opera√ß√µes cr√≠ticas:**

```python
import time
from enum import Enum
from typing import Callable, Any

class EstadoCircuit(Enum):
    FECHADO = "fechado"
    ABERTO = "aberto"
    MEIO_ABERTO = "meio_aberto"

class CircuitBreaker:
    def __init__(self, limite_falhas: int = 5, timeout_recuperacao: int = 60):
        self.limite_falhas = limite_falhas
        self.timeout_recuperacao = timeout_recuperacao
        self.contador_falhas = 0
        self.tempo_ultima_falha = None
        self.estado = EstadoCircuit.FECHADO
    
    def executar(self, func: Callable, *args, **kwargs) -> Any:
        """Executar fun√ß√£o com circuit breaker"""
        if self.estado == EstadoCircuit.ABERTO:
            if time.time() - self.tempo_ultima_falha > self.timeout_recuperacao:
                self.estado = EstadoCircuit.MEIO_ABERTO
            else:
                raise Exception("Circuit breaker est√° aberto")
        
        try:
            resultado = func(*args, **kwargs)
            self.ao_sucesso()
            return resultado
        except Exception as e:
            self.ao_falha()
            raise e
    
    def ao_sucesso(self):
        """Reset circuit breaker no sucesso"""
        self.contador_falhas = 0
        self.estado = EstadoCircuit.FECHADO
    
    def ao_falha(self):
        """Incrementar contador de falhas"""
        self.contador_falhas += 1
        self.tempo_ultima_falha = time.time()
        
        if self.contador_falhas >= self.limite_falhas:
            self.estado = EstadoCircuit.ABERTO

# üîÑ Uso pr√°tico
circuit_breaker = CircuitBreaker(limite_falhas=3, timeout_recuperacao=30)

def operacao_instavel():
    # Simular opera√ß√£o que pode falhar
    import random
    if random.random() < 0.7:  # 70% chance de falha
        raise Exception("Opera√ß√£o falhou")
    return "Sucesso!"

# Executar com circuit breaker
try:
    resultado = circuit_breaker.executar(operacao_instavel)
    print(f"Resultado: {resultado}")
except Exception as e:
    print(f"Erro: {e}")
```

### Monitoramento e logging estruturado

```python
import json
import logging
import time
from datetime import datetime

class LoggerEstruturado:
    def __init__(self, nome: str):
        self.logger = logging.getLogger(nome)
        self.configurar_logger()
    
    def configurar_logger(self):
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def log_estruturado(self, level: str, mensagem: str, **kwargs):
        """Gerar log estruturado em JSON"""
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': level.upper(),
            'message': mensagem,
            'service': 'docker-manager',
            **kwargs
        }
        
        getattr(self.logger, level.lower())(json.dumps(log_entry))

# üìä Monitoramento de containers
class MonitorContainer:
    def __init__(self):
        self.client = docker.from_env()
        self.logger = LoggerEstruturado(__name__)
    
    def coletar_metricas_container(self, container_id: str):
        """Coletar m√©tricas detalhadas do container"""
        try:
            container = self.client.containers.get(container_id)
            stats = container.stats(stream=False)
            
            # Calcular CPU usage
            cpu_stats = stats['cpu_stats']
            precpu_stats = stats['precpu_stats']
            cpu_delta = cpu_stats['cpu_usage']['total_usage'] - precpu_stats['cpu_usage']['total_usage']
            system_delta = cpu_stats['system_cpu_usage'] - precpu_stats['system_cpu_usage']
            cpu_usage = (cpu_delta / system_delta) * len(cpu_stats['cpu_usage']['percpu_usage']) * 100.0
            
            # Memory usage
            memory_stats = stats['memory_stats']
            memory_usage = (memory_stats['usage'] / memory_stats['limit']) * 100.0
            
            metricas = {
                'container_id': container_id,
                'container_name': container.name,
                'cpu_usage_percent': round(cpu_usage, 2),
                'memory_usage_percent': round(memory_usage, 2),
                'memory_usage_mb': round(memory_stats['usage'] / (1024*1024), 2),
                'memory_limit_mb': round(memory_stats['limit'] / (1024*1024), 2),
                'status': container.status
            }
            
            # Log m√©tricas
            self.logger.log_estruturado('info', 'M√©tricas container coletadas', **metricas)
            
            # Alertas
            if cpu_usage > 80:
                self.logger.log_estruturado('warning', 'Alto uso CPU', container_id=container_id, cpu_usage=cpu_usage)
            if memory_usage > 85:
                self.logger.log_estruturado('warning', 'Alto uso mem√≥ria', container_id=container_id, memory_usage=memory_usage)
            
            return metricas
            
        except Exception as e:
            self.logger.log_estruturado('error', 'Erro coletando m√©tricas', container_id=container_id, erro=str(e))
            raise
```

## Exemplos pr√°ticos completos

### Sistema completo langchain-ollama + WAHA + Docker

```python
import asyncio
import docker
from langchain_ollama import OllamaLLM
from tenacity import retry, stop_after_attempt, wait_exponential
import aiohttp
import logging

class SistemaCompleto:
    def __init__(self):
        self.docker_client = docker.from_env()
        self.logger = logging.getLogger(__name__)
        self.ollama_llm = None
        self.waha_client = None
        
    async def inicializar_sistema(self):
        """Inicializar todos os componentes"""
        # 1. Verificar/iniciar Ollama
        await self.garantir_ollama_disponivel()
        
        # 2. Inicializar LLM
        self.ollama_llm = OllamaLLM(
            model="llama3.1",
            temperature=0.1,
            num_ctx=4096,
            client_kwargs={"timeout": 120.0},
            keep_alive="30m",
            validate_model_on_init=True
        )
        
        # 3. Verificar/iniciar WAHA
        await self.garantir_waha_disponivel()
        
        # 4. Configurar cliente WAHA
        self.waha_client = WAHAClient('http://localhost:3000', 'sua-api-key')
        
        self.logger.info("Sistema completo inicializado")
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=5, max=30))
    async def garantir_ollama_disponivel(self):
        """Garantir que Ollama est√° rodando"""
        try:
            # Verificar se container Ollama existe
            containers = self.docker_client.containers.list(filters={'name': 'ollama'})
            
            if not containers:
                # Iniciar container Ollama
                self.logger.info("Iniciando container Ollama")
                container = self.docker_client.containers.run(
                    'ollama/ollama:latest',
                    name='ollama',
                    ports={'11434/tcp': 11434},
                    volumes={'ollama_data': {'bind': '/root/.ollama', 'mode': 'rw'}},
                    detach=True,
                    restart_policy={"Name": "unless-stopped"}
                )
                
                # Aguardar Ollama ficar dispon√≠vel
                await self.aguardar_servico_disponivel('http://localhost:11434/api/tags')
                
                # Baixar modelo se necess√°rio
                await self.garantir_modelo_disponivel('llama3.1')
        
        except Exception as e:
            self.logger.error(f"Erro garantindo Ollama: {e}")
            raise
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=5, max=30))
    async def garantir_waha_disponivel(self):
        """Garantir que WAHA est√° rodando"""
        try:
            containers = self.docker_client.containers.list(filters={'name': 'waha'})
            
            if not containers:
                self.logger.info("Iniciando container WAHA")
                container = self.docker_client.containers.run(
                    'devlikeapro/waha:latest',
                    name='waha',
                    ports={'3000/tcp': 3000},
                    environment={
                        'WAHA_API_KEY': 'sua-api-key',
                        'WHATSAPP_DEFAULT_ENGINE': 'WEBJS'
                    },
                    volumes={'./sessions': {'bind': '/app/.sessions', 'mode': 'rw'}},
                    detach=True,
                    restart_policy={"Name": "unless-stopped"}
                )
                
                # Aguardar WAHA ficar dispon√≠vel
                await self.aguardar_servico_disponivel('http://localhost:3000/health')
        
        except Exception as e:
            self.logger.error(f"Erro garantindo WAHA: {e}")
            raise
    
    async def aguardar_servico_disponivel(self, url: str, timeout: int = 60):
        """Aguardar servi√ßo ficar dispon√≠vel"""
        import time
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(url) as response:
                        if response.status == 200:
                            return True
            except:
                pass
            
            await asyncio.sleep(5)
        
        raise TimeoutError(f"Servi√ßo {url} n√£o ficou dispon√≠vel")
    
    async def processar_mensagem_whatsapp(self, chat_id: str, mensagem: str):
        """Processar mensagem do WhatsApp com IA"""
        try:
            # Processar com Ollama
            resposta_ia = await self.ollama_llm.ainvoke(
                f"Responda de forma √∫til e concisa: {mensagem}"
            )
            
            # Enviar resposta via WAHA
            self.waha_client.enviar_texto(chat_id, resposta_ia)
            
            self.logger.info(f"Mensagem processada para {chat_id}")
            return resposta_ia
            
        except Exception as e:
            self.logger.error(f"Erro processando mensagem: {e}")
            # Enviar mensagem de erro
            self.waha_client.enviar_texto(chat_id, "Desculpe, houve um erro interno.")
            raise

# üöÄ Uso do sistema completo
async def main():
    sistema = SistemaCompleto()
    await sistema.inicializar_sistema()
    
    # Exemplo de uso
    resposta = await sistema.processar_mensagem_whatsapp(
        "5511999999999@c.us", 
        "Explique o que √© intelig√™ncia artificial"
    )
    print(f"Resposta enviada: {resposta[:100]}...")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
```

Este guia fornece informa√ß√µes t√©cnicas precisas e exemplos pr√°ticos para implementar uma solu√ß√£o robusta usando langchain-ollama, WAHA e Docker com Python, incluindo retry autom√°tico, tratamento de erros e melhores pr√°ticas de produ√ß√£o.